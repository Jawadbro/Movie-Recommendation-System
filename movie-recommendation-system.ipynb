{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1872300,"sourceType":"datasetVersion","datasetId":1114664}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n================================================================================\nMOVIE RECOMMENDATION SYSTEM - MovieLens 1M Dataset\n================================================================================\nThis notebook implements and compares five recommendation algorithms:\n1. Random Baseline\n2. Popularity-Based\n3. Item-Based Collaborative Filtering (ItemCF)\n4. Matrix Factorization (SVD)\n5. Neural Collaborative Filtering (Enhanced)\n\nDataset: MovieLens 1M (1 million ratings from 6,040 users on 3,883 movies)\nBest Performance: ItemCF with 32.1% Precision@10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:48:29.998496Z","iopub.execute_input":"2025-10-04T14:48:29.999058Z","iopub.status.idle":"2025-10-04T14:48:32.430741Z","shell.execute_reply.started":"2025-10-04T14:48:29.999034Z","shell.execute_reply":"2025-10-04T14:48:32.429967Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# 1. IMPORTS AND SETUP\n# ============================================================================\n\"\"\"\nImport required libraries for data processing, modeling, and evaluation.\n- pandas/numpy: Data manipulation\n- torch: Neural network implementation\n- surprise: Collaborative filtering algorithms\n- sklearn: Train-test split functionality\n\"\"\"\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix\nfrom surprise import Dataset, Reader, SVD, KNNBasic\nfrom collections import defaultdict\n\n# Check dataset files and set data path\nos.listdir(\"/kaggle/input/movielens-1m-dataset\")\nDATA_PATH = \"/kaggle/input/movielens-1m-dataset\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 2. DATA LOADING\n# ============================================================================\n\"\"\"\nLoad the three MovieLens 1M dataset files:\n- movies.dat: Movie information (ID, title, genres)\n- ratings.dat: User-movie ratings (user_id, movie_id, rating, timestamp)\n- users.dat: User demographics (ID, gender, age, occupation, zip code)\n\nNote: Files use '::' as delimiter and require latin-1 encoding\n\"\"\"\n# Load movies data\nmovies = pd.read_csv(f\"{DATA_PATH}/movies.dat\", sep=\"::\", engine='python',\n                     names=['movie_id', 'title', 'genres'], encoding='latin-1')\n\n\n# Load ratings data\nratings = pd.read_csv(f\"{DATA_PATH}/ratings.dat\", sep=\"::\", engine='python',\n                      names=['user_id', 'movie_id', 'rating', 'timestamp'], encoding='latin-1')\n\n# Load users data\nusers = pd.read_csv(f\"{DATA_PATH}/users.dat\", sep=\"::\", engine='python',\n                    names=['user_id', 'gender', 'age', 'occupation', 'zip_code'], encoding='latin-1')\n\nprint(\"Movies:\", movies.shape) # Expected: (3883, 3)\nprint(\"Ratings:\", ratings.shape) # Expected: (1000209, 4)\nprint(\"Users:\", users.shape) # Expected: (6040, 5)\n\n# Check first rows to verify data loaded correctly\nmovies.head()\nratings.head()\nusers.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:49:22.040982Z","iopub.execute_input":"2025-10-04T14:49:22.041921Z","iopub.status.idle":"2025-10-04T14:49:26.760832Z","shell.execute_reply.started":"2025-10-04T14:49:22.041894Z","shell.execute_reply":"2025-10-04T14:49:26.759910Z"}},"outputs":[{"name":"stdout","text":"Movies: (3883, 3)\nRatings: (1000209, 4)\nUsers: (6040, 5)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   user_id gender  age  occupation zip_code\n0        1      F    1          10    48067\n1        2      M   56          16    70072\n2        3      M   25          15    55117\n3        4      M   45           7    02460\n4        5      M   25          20    55455","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>occupation</th>\n      <th>zip_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>M</td>\n      <td>56</td>\n      <td>16</td>\n      <td>70072</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>M</td>\n      <td>25</td>\n      <td>15</td>\n      <td>55117</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>M</td>\n      <td>45</td>\n      <td>7</td>\n      <td>02460</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>M</td>\n      <td>25</td>\n      <td>20</td>\n      <td>55455</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# 3. TRAIN-TEST SPLIT\n# ============================================================================\n\"\"\"\nSplit ratings into training (80%) and testing (20%) sets.\n- Training set: Used to train all recommendation models\n- Test set: Used to evaluate model performance\n- Random state: Set to 42 for reproducibility\n\"\"\"\ntrain_ratings, test_ratings = train_test_split(\n    ratings, \n    test_size=0.2, # 20% for testing\n    random_state=42 # Ensures reproducible splits\n)\n\n# Create dictionary mapping user_id to list of movies they interacted with in test set\n# This is used for efficient evaluation later\ntest_by_user = defaultdict(list)\nfor _, row in test_ratings.iterrows():\n    test_by_user[row['user_id']].append(row['movie_id'])\n\nprint(\"Train ratings:\", train_ratings.shape)\nprint(\"Test ratings:\", test_ratings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:50:05.805101Z","iopub.execute_input":"2025-10-04T14:50:05.805775Z","iopub.status.idle":"2025-10-04T14:50:10.962957Z","shell.execute_reply.started":"2025-10-04T14:50:05.805747Z","shell.execute_reply":"2025-10-04T14:50:10.962054Z"}},"outputs":[{"name":"stdout","text":"Train ratings: (800167, 4)\nTest ratings: (200042, 4)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# 4. BASELINE MODELS - ITEM-BASED COLLABORATIVE FILTERING (SKLEARN)\n# ============================================================================\n# Unique users and movies in train\ntrain_user_ids = train_ratings['user_id'].unique()\ntrain_movie_ids = train_ratings['movie_id'].unique()\n\nuser_map_train = {old:new for new,old in enumerate(sorted(train_user_ids))}\nmovie_map_train = {old:new for new,old in enumerate(sorted(train_movie_ids))}\n\ntrain_ratings['user_idx'] = train_ratings['user_id'].map(user_map_train)\ntrain_ratings['movie_idx'] = train_ratings['movie_id'].map(movie_map_train)\n\nn_users_train = len(user_map_train)\nn_items_train = len(movie_map_train)\n\n# Create sparse matrix\nR_train = csr_matrix(\n    (train_ratings['rating'], (train_ratings['user_idx'], train_ratings['movie_idx'])),\n    shape=(n_users_train, n_items_train)\n)\n\n# Compute item-item cosine similarity\nitem_similarity_train = cosine_similarity(R_train.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:51:10.471331Z","iopub.execute_input":"2025-10-04T14:51:10.471668Z","iopub.status.idle":"2025-10-04T14:51:11.630481Z","shell.execute_reply.started":"2025-10-04T14:51:10.471648Z","shell.execute_reply":"2025-10-04T14:51:11.629650Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# 5. BASELINE MODELS - POPULARITY\n# ============================================================================\n# Compute movie popularity based on training ratings\nmovie_popularity = train_ratings.groupby('movie_id')['rating'].sum().sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:51:29.894703Z","iopub.execute_input":"2025-10-04T14:51:29.895213Z","iopub.status.idle":"2025-10-04T14:51:29.913223Z","shell.execute_reply.started":"2025-10-04T14:51:29.895187Z","shell.execute_reply":"2025-10-04T14:51:29.912715Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# 5. COLLABORATIVE FILTERING MODELS (SURPRISE) \n# ============================================================================\n\"\"\"\nPrepare and train collaborative filtering models using Surprise library:\n1. Item-Based Collaborative Filtering (ItemCF)\n2. Singular Value Decomposition (SVD)\n\nSurprise requires data in specific format:\n- Reader: Defines rating scale (1-5 for MovieLens)\n- Dataset: Wraps training data\n- Trainset: Internal format for model training\n\"\"\"\n# Prepare Surprise Dataset\nreader = Reader(rating_scale=(1,5))\ntrain_data = Dataset.load_from_df(train_ratings[['user_id','movie_id','rating']], reader)\ntrainset = train_data.build_full_trainset()\n\n# Item-based Collaborative Filtering \nprint(\"\\nTraining Item-based CF with improved parameters...\")\n\n# Try multiple configurations\nconfigurations = [\n    {\n        'name': 'ItemCF_Cosine_k50_minsup1',\n        'sim_options': {\n            'name': 'cosine',\n            'user_based': False,\n            'min_support': 1  # Reduced from 5 to capture more item pairs\n        },\n        'k': 50  # Increased from 40\n    },\n    {\n        'name': 'ItemCF_Pearson_k40_minsup2',\n        'sim_options': {\n            'name': 'pearson',  # Better for handling rating scale differences\n            'user_based': False,\n            'min_support': 2\n        },\n        'k': 40\n    },\n    {\n        'name': 'ItemCF_Pearson_k80_minsup1',\n        'sim_options': {\n            'name': 'pearson',\n            'user_based': False,\n            'min_support': 1\n        },\n        'k': 80  # More neighbors for better coverage\n    }\n]\n\n# Train all configurations and select best\nprint(\"Testing multiple ItemCF configurations...\")\nitemcf_models = {}\n\nfor config in configurations:\n    print(f\"  Training {config['name']}...\")\n    model = KNNBasic(k=config['k'], sim_options=config['sim_options'])\n    model.fit(trainset)\n    itemcf_models[config['name']] = model\n\n# Use the Pearson with k=80 as default (typically performs best)\nitemcf = itemcf_models['ItemCF_Pearson_k80_minsup1']\nprint(f\"✓ ItemCF trained with improved parameters\")\n\n# Matrix Factorization - SVD \n\"\"\"\nSVD decomposes user-item rating matrix into latent factors.\nConfiguration:\n- n_factors=150: Dimensionality of latent factor space\n- n_epochs=30: Number of training iterations\n- random_state=42: For reproducibility\n\nSVD is good at rating prediction but may not excel at ranking.\n\"\"\"\nprint(\"Training SVD...\")\nsvd = SVD(\n    n_factors=150,\n    n_epochs=30,\n    lr_all=0.005,\n    reg_all=0.02,\n    random_state=42\n)\nsvd.fit(trainset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:51:54.376663Z","iopub.execute_input":"2025-10-04T14:51:54.376943Z","iopub.status.idle":"2025-10-04T14:53:06.201072Z","shell.execute_reply.started":"2025-10-04T14:51:54.376925Z","shell.execute_reply":"2025-10-04T14:53:06.200308Z"}},"outputs":[{"name":"stdout","text":"\nTraining Item-based CF with improved parameters...\nTesting multiple ItemCF configurations...\n  Training ItemCF_Cosine_k50_minsup1...\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\n  Training ItemCF_Pearson_k40_minsup2...\nComputing the pearson similarity matrix...\nDone computing similarity matrix.\n  Training ItemCF_Pearson_k80_minsup1...\nComputing the pearson similarity matrix...\nDone computing similarity matrix.\n✓ ItemCF trained with improved parameters\nTraining SVD...\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x78363a7d1f10>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# 7. NEURAL COLLABORATIVE FILTERING\n# ============================================================================\n\"\"\"\nImplement deep learning-based recommendation using Neural Collaborative Filtering.\nArchitecture: GMF (Generalized Matrix Factorization) + MLP (Multi-Layer Perceptron)\nReference: He et al. \"Neural Collaborative Filtering\" (WWW 2017)\n\nSteps:\n1. Map user/movie IDs to continuous indices\n2. Create PyTorch Dataset and DataLoader\n3. Define neural network architecture\n4. Train with Adam optimizer and MSE loss\n\"\"\"\n# Map user/movie IDs to indices\nuser_ids = train_ratings['user_id'].unique()\nmovie_ids = train_ratings['movie_id'].unique()\n\nuser2idx = {uid: idx for idx, uid in enumerate(user_ids)}\nmovie2idx = {mid: idx for idx, mid in enumerate(movie_ids)}\n\n# Apply mappings\ntrain_ratings_nn = train_ratings.copy()\ntrain_ratings_nn['user_idx'] = train_ratings_nn['user_id'].map(user2idx)\ntrain_ratings_nn['movie_idx'] = train_ratings_nn['movie_id'].map(movie2idx)\n\n# Dataset class\nclass MovieLensDataset(Dataset):\n    def __init__(self, df):\n        self.users = torch.tensor(df['user_idx'].values, dtype=torch.long)\n        self.movies = torch.tensor(df['movie_idx'].values, dtype=torch.long)\n        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n        \n    def __len__(self):\n        return len(self.ratings)\n    \n    def __getitem__(self, idx):\n        return self.users[idx], self.movies[idx], self.ratings[idx]\n\ntrain_dataset = MovieLensDataset(train_ratings_nn)\ntrain_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)  \n\n# Neural CF Model with GMF + MLP fusion (like original NCF paper)\n\"\"\"\n    Enhanced Neural Collaborative Filtering with GMF + MLP fusion.\n    \n    Architecture:\n    - GMF path: Element-wise product of user/item embeddings (captures linear interactions)\n    - MLP path: Deep network on concatenated embeddings (captures non-linear interactions)\n    - Fusion layer: Combines GMF and MLP outputs for final prediction\n    \n    Args:\n        n_users: Number of unique users\n        n_items: Number of unique items\n        emb_size: Embedding dimension (default: 200)\n        hidden_layers: List of hidden layer sizes (default: [512, 256, 128, 64])\n    \"\"\"\nclass EnhancedNeuralCF(nn.Module):\n    def __init__(self, n_users, n_items, emb_size=200, hidden_layers=[512, 256, 128, 64]):\n        super().__init__()\n        \n        # GMF (Generalized Matrix Factorization) path\n        self.user_emb_gmf = nn.Embedding(n_users, emb_size)\n        self.item_emb_gmf = nn.Embedding(n_items, emb_size)\n        \n        # MLP path\n        self.user_emb_mlp = nn.Embedding(n_users, emb_size)\n        self.item_emb_mlp = nn.Embedding(n_items, emb_size)\n        \n        # MLP layers\n        self.bn1 = nn.BatchNorm1d(emb_size * 2)\n        layers = []\n        input_size = emb_size * 2\n        for h in hidden_layers:\n            layers.append(nn.Linear(input_size, h))\n            layers.append(nn.ReLU())\n            layers.append(nn.BatchNorm1d(h))\n            layers.append(nn.Dropout(0.25))\n            input_size = h\n        self.mlp = nn.Sequential(*layers)\n        \n        # Fusion layer\n        self.fusion = nn.Linear(emb_size + hidden_layers[-1], 1)\n        \n        self._init_weights()\n        \n    def _init_weights(self):\n        #initialization\n        nn.init.normal_(self.user_emb_gmf.weight, std=0.01)\n        nn.init.normal_(self.item_emb_gmf.weight, std=0.01)\n        nn.init.normal_(self.user_emb_mlp.weight, std=0.01)\n        nn.init.normal_(self.item_emb_mlp.weight, std=0.01)\n        \n    def forward(self, user, item):\n        # GMF path (element-wise multiplication)\n        u_gmf = self.user_emb_gmf(user)\n        i_gmf = self.item_emb_gmf(item)\n        gmf_out = u_gmf * i_gmf\n        \n        # MLP path (concatenation)\n        u_mlp = self.user_emb_mlp(user)\n        i_mlp = self.item_emb_mlp(item)\n        mlp_input = torch.cat([u_mlp, i_mlp], dim=-1)\n        mlp_input = self.bn1(mlp_input)\n        mlp_out = self.mlp(mlp_input)\n        \n        # Fusion\n        fusion_input = torch.cat([gmf_out, mlp_out], dim=-1)\n        output = self.fusion(fusion_input)\n        \n        return output.squeeze()\n\n# Train Neural Model\n\"\"\"\nTrain the neural model using:\n- Optimizer: Adam with weight decay (L2 regularization)\n- Loss: Mean Squared Error (MSE) for rating prediction\n- Learning rate scheduler: Reduces LR when loss plateaus\n- Gradient clipping: Prevents exploding gradients\n\"\"\"\nprint(\"\\nTraining Enhanced Neural CF...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nneural_model = EnhancedNeuralCF(  \n    len(user2idx), \n    len(movie2idx), \n    emb_size=200,  \n    hidden_layers=[512, 256, 128, 64]\n).to(device)\n\noptimizer = torch.optim.Adam(neural_model.parameters(), lr=0.001, weight_decay=1e-5) \ncriterion = nn.MSELoss()\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n)\n\nepochs = 60  \nbest_loss = float('inf')\npatience_counter = 0\nearly_stop_patience = 10\n\nfor epoch in range(epochs):\n    neural_model.train() \n    total_loss = 0\n    for u, m, r in train_loader:\n        u, m, r = u.to(device), m.to(device), r.to(device)\n        optimizer.zero_grad()\n        pred = neural_model(u, m) \n        loss = criterion(pred, r)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(neural_model.parameters(), max_norm=1.0)  \n        \n        optimizer.step()\n        total_loss += loss.item() * len(r)\n    \n    avg_loss = total_loss / len(train_dataset)\n    rmse = np.sqrt(avg_loss)\n    \n    # Update learning rate\n    scheduler.step(rmse)\n    \n    if (epoch + 1) % 5 == 0:\n        print(f\"Epoch {epoch+1}: Train RMSE = {rmse:.4f}\")\n    \n    # Early stopping check\n    if rmse < best_loss:\n        best_loss = rmse\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stop_patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(f\"✓ Enhanced Neural CF trained. Best RMSE: {best_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:13:19.191588Z","iopub.execute_input":"2025-10-04T15:13:19.191902Z","iopub.status.idle":"2025-10-04T15:30:19.379031Z","shell.execute_reply.started":"2025-10-04T15:13:19.191879Z","shell.execute_reply":"2025-10-04T15:30:19.377942Z"}},"outputs":[{"name":"stdout","text":"\nTraining Enhanced Neural CF...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train RMSE = 0.9090\nEpoch 10: Train RMSE = 0.7981\nEpoch 15: Train RMSE = 0.6974\nEpoch 20: Train RMSE = 0.6360\nEpoch 25: Train RMSE = 0.5902\nEpoch 30: Train RMSE = 0.5504\nEpoch 35: Train RMSE = 0.5121\nEpoch 40: Train RMSE = 0.4769\nEpoch 45: Train RMSE = 0.4505\nEpoch 50: Train RMSE = 0.4331\nEpoch 55: Train RMSE = 0.4213\nEpoch 60: Train RMSE = 0.4121\n✓ Enhanced Neural CF trained. Best RMSE: 0.4121\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# 8. RECOMMENDATION FUNCTIONS (FOR EVALUATION)\n# ============================================================================\n\"\"\"\nDefine recommendation functions for each model.\nAll functions follow the same interface:\n- Input: user_id, N (number of recommendations)\n- Output: List of N movie IDs\n- Behavior: Exclude movies the user has already seen\n\"\"\"\ndef recommend_random(user_id, N=10):\n    \"\"\"\n    Random baseline: Recommend N random unseen movies.\n    \n    This serves as the worst-case baseline. Any reasonable model should\n    significantly outperform random recommendations.\n    \n    Args:\n        user_id: User ID to generate recommendations for\n        N: Number of recommendations to return\n        \n    Returns:\n        List of N randomly selected movie IDs\n        \"\"\"\n    seen = set(train_ratings[train_ratings['user_id']==user_id]['movie_id'])\n    candidates = list(set(train_movie_ids)-seen)\n    chosen = np.random.choice(candidates, size=min(N,len(candidates)), replace=False)\n    return [(mid, movies[movies['movie_id']==mid]['title'].values[0]) for mid in chosen]\n\ndef recommend_popular(user_id, N=10):\n     \"\"\"\n    Popularity baseline: Recommend N most popular unseen movies.\n    \n    \"Popular\" means movies with the most ratings. This is a simple but\n    effective non-personalized baseline.\n    \n    Args:\n        user_id: User ID to generate recommendations for\n        N: Number of recommendations to return\n        \n    Returns:\n        List of N most popular movie IDs (excluding already seen)\n    \"\"\"\n    movie_counts_train = train_ratings.groupby('movie_id').size().sort_values(ascending=False)\n    seen = set(train_ratings[train_ratings['user_id']==user_id]['movie_id'])\n    recs = []\n    for mid in movie_counts_train.index:\n        if mid not in seen:\n            recs.append((mid, movies[movies['movie_id']==mid]['title'].values[0]))\n        if len(recs) >= N:\n            break\n    return recs\n\ndef recommend_itemcf_train(user_id, N=10, top_k=20):\n     \"\"\"\n    Item-based Collaborative Filtering recommendations.\n    \n    Uses item-item similarity to find movies similar to those the user liked.\n    Falls back to popularity if unable to generate predictions.\n    \n    Args:\n        user_id: User ID to generate recommendations for\n        N: Number of recommendations to return\n        \n    Returns:\n        List of N movie IDs ranked by predicted rating\n    \"\"\"\n    if user_id not in user_map_train:\n        return []\n    uidx = user_map_train[user_id]\n    user_rated = train_ratings[train_ratings['user_idx']==uidx][['movie_idx','rating']]\n    \n    scores = np.zeros(n_items_train)\n    for _, row in user_rated.iterrows():\n        midx = row['movie_idx']\n        rating = row['rating']\n        sim_scores = item_similarity_train[midx]\n        top_neighbors = np.argsort(sim_scores)[::-1][:top_k]\n        scores[top_neighbors] += sim_scores[top_neighbors]*rating\n\n    seen = set(user_rated['movie_idx'])\n    scores[list(seen)] = -1e9\n    \n    top_items = np.argsort(scores)[::-1][:N]\n    train_movie_list = sorted(train_movie_ids)\n    return [(train_movie_list[i], movies[movies['movie_id']==train_movie_list[i]]['title'].values[0]) for i in top_items]\n\ndef recommend_svd(user_id, N=10):\n    \"\"\"\n    SVD Matrix Factorization recommendations.\n    \n    Uses latent factor model to predict ratings for unseen movies.\n    \n    Args:\n        user_id: User ID to generate recommendations for\n        N: Number of recommendations to return\n        \n    Returns:\n        List of N movie IDs ranked by predicted rating\n    \"\"\"\n    seen = set(train_ratings[train_ratings['user_id']==user_id]['movie_id'])\n    candidate_movies = set(ratings['movie_id']) - seen\n    preds = []\n    for mid in candidate_movies:\n        est = svd.predict(user_id, mid).est\n        preds.append((mid, est))\n    preds.sort(key=lambda x: x[1], reverse=True)\n    topN = preds[:N]\n    return [(mid, movies[movies['movie_id']==mid]['title'].values[0]) for mid,_ in topN]\n\ndef recommend_neural(user_id, N=10):\n     \"\"\"\n    Neural Collaborative Filtering recommendations.\n    \n    Uses deep learning model to predict ratings. Generates predictions for\n    all movies in batch for efficiency.\n    \n    Args:\n        user_id: User ID to generate recommendations for\n        N: Number of recommendations to return\n        \n    Returns:\n        List of N movie IDs ranked by predicted rating\n    \"\"\"\n  \n    neural_model.eval()\n    \n    if user_id not in user2idx:\n        return []\n    \n    uid = torch.tensor([user2idx[user_id]]*len(movie2idx)).to(device)\n    mid = torch.tensor(list(range(len(movie2idx)))).to(device)\n    \n    with torch.no_grad():\n        preds = neural_model(uid, mid).cpu().numpy()\n    \n    idx2movie = {v: k for k, v in movie2idx.items()}\n    \n    seen = set(train_ratings[train_ratings['user_id'] == user_id]['movie_id'])\n    recommendations = [(idx2movie[i], p) for i, p in enumerate(preds) if idx2movie[i] not in seen]\n    recommendations.sort(key=lambda x: x[1], reverse=True)\n    \n    return [mid for mid, _ in recommendations[:N]]\n\ndef recommend_movies_eval(user_id, N=10, method=\"itemcf\"):\n    if method==\"itemcf\":\n        return recommend_itemcf_train(user_id,N)\n    elif method==\"svd\":\n        return recommend_svd(user_id,N)\n    elif method==\"popular\":\n        return recommend_popular(user_id,N)\n    elif method==\"random\":\n        return recommend_random(user_id,N)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:03:25.830304Z","iopub.execute_input":"2025-10-04T15:03:25.830912Z","iopub.status.idle":"2025-10-04T15:03:25.843905Z","shell.execute_reply.started":"2025-10-04T15:03:25.830887Z","shell.execute_reply":"2025-10-04T15:03:25.843085Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# 9. EVALUATION FUNCTIONS\n# ============================================================================\n\"\"\"\nImplement standard evaluation metrics for top-N recommendation:\n\n1. Precision@K: Fraction of recommended items that are relevant\n   - Higher is better (max 1.0)\n   - Measures recommendation accuracy\n   \n2. Recall@K: Fraction of relevant items that are recommended\n   - Higher is better (max 1.0)\n   - Measures coverage of relevant items\n   \n3. NDCG@K: Normalized Discounted Cumulative Gain\n   - Higher is better (max 1.0)\n   - Measures ranking quality (penalizes relevant items lower in list)\n\"\"\"\ndef recommend_movies(user_id, N=10, method=\"popular\"):\n    \"\"\"\n    Unified recommendation function with movie titles\n    \"\"\"\n    # Get recommendations\n    if method.lower() == \"random\":\n        recs = recommend_random(user_id, N=N)\n    elif method.lower() == \"popular\":\n        recs = recommend_popular(user_id, N=N)\n    elif method.lower() == \"itemcf\":\n        recs = recommend_itemcf_train(user_id, N=N)\n    elif method.lower() == \"svd\":\n        recs = recommend_svd(user_id, N=N)\n    elif method.lower() == \"neural\":\n        if user_id not in user2idx:\n            return []\n        recs = recommend_neural(user_id, N=N)\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    # Ensure recs is a flat list\n    if isinstance(recs, np.ndarray):\n        recs = recs.flatten().tolist()\n    \n    # Build result with movie titles\n    result = []\n    for item in recs:\n        # Handle different return formats\n        if isinstance(item, (tuple, list)):\n            mid = item[0]  # If it's (movie_id, score)\n        else:\n            mid = item  # If it's just movie_id\n        \n        # Convert to int if needed\n        if isinstance(mid, np.ndarray):\n            mid = int(mid.flatten()[0])\n        else:\n            mid = int(mid)\n        \n        # Get movie title\n        try:\n            movie_row = movies[movies['movie_id'] == mid]\n            if not movie_row.empty:\n                title = movie_row['title'].values[0]\n                result.append((mid, title))\n        except:\n            continue\n    \n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:36:29.146937Z","iopub.execute_input":"2025-10-04T15:36:29.147430Z","iopub.status.idle":"2025-10-04T15:36:29.154402Z","shell.execute_reply.started":"2025-10-04T15:36:29.147401Z","shell.execute_reply":"2025-10-04T15:36:29.153628Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# 10. EVALUATION FUNCTIONS\n# ============================================================================\ndef evaluate_model(method, k=10, n_users=500, wrapper=recommend_movies_eval):\n    precisions, recalls, ndcgs = [], [], []\n    sampled_users = np.random.choice(list(test_by_user.keys()), size=min(n_users,len(test_by_user)), replace=False)\n    \n    for uid in sampled_users:\n        relevant = test_by_user[uid]\n        recs = wrapper(uid,N=k,method=method)\n        rec_ids = [mid for mid,_ in recs]\n        hits = len(set(rec_ids) & set(relevant))\n        precisions.append(hits/k)\n        recalls.append(hits/len(relevant) if len(relevant)>0 else 0)\n        dcg = sum(1/np.log2(i+2) for i,r in enumerate(rec_ids) if r in relevant)\n        idcg = sum(1/np.log2(i+2) for i in range(min(len(relevant),k)))\n        ndcgs.append(dcg/idcg if idcg>0 else 0)\n    \n    return {\"Precision@K\":np.mean(precisions),\n            \"Recall@K\":np.mean(recalls),\n            \"NDCG@K\":np.mean(ndcgs)}\n\ndef evaluate_neural(N=10):\n    \"\"\"Evaluate Neural CF\"\"\"\n    precision_list = []\n    recall_list = []\n    ndcg_list = []\n    \n    for user_id in test_by_user.keys():\n        if user_id not in user2idx:\n            continue\n            \n        actual_items = set(test_by_user[user_id])\n        if len(actual_items) == 0:\n            continue\n        \n        try:\n            pred_items = recommend_neural(user_id, N=N)  # Returns list of movie IDs\n        except:\n            continue\n        \n        if len(pred_items) == 0:\n            continue\n        \n        # pred_items is already a list of integers, no unpacking needed\n        pred_items_set = set(pred_items)\n        hits = len(pred_items_set & actual_items)\n        \n        precision = hits / N\n        recall = hits / len(actual_items)\n        \n        dcg = sum([1 / np.log2(i+2) for i, item in enumerate(pred_items) if item in actual_items])\n        idcg = sum([1 / np.log2(i+2) for i in range(min(len(actual_items), N))])\n        ndcg = dcg / idcg if idcg > 0 else 0\n        \n        precision_list.append(precision)\n        recall_list.append(recall)\n        ndcg_list.append(ndcg)\n    \n    return {\n        'Precision@K': np.mean(precision_list) if precision_list else 0,\n        'Recall@K': np.mean(recall_list) if recall_list else 0,\n        'NDCG@K': np.mean(ndcg_list) if ndcg_list else 0\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:33:14.804997Z","iopub.execute_input":"2025-10-04T15:33:14.805614Z","iopub.status.idle":"2025-10-04T15:33:14.815450Z","shell.execute_reply.started":"2025-10-04T15:33:14.805585Z","shell.execute_reply":"2025-10-04T15:33:14.814692Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ============================================================================\n# 11. EVALUATION RESULTS\n# ============================================================================\nprint(\"\\n\" + \"=\"*50)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(\"Random:\", evaluate_model(\"random\",k=10))\nprint(\"Popularity:\", evaluate_model(\"popular\",k=10))\nprint(\"ItemCF:\", evaluate_model(\"itemcf\",k=10))\nprint(\"SVD:\", evaluate_model(\"svd\",k=10))\nprint(\"Neural:\", evaluate_neural(N=10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:33:26.373784Z","iopub.execute_input":"2025-10-04T15:33:26.374368Z","iopub.status.idle":"2025-10-04T15:35:25.711929Z","shell.execute_reply.started":"2025-10-04T15:33:26.374347Z","shell.execute_reply":"2025-10-04T15:35:25.711187Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nEVALUATION RESULTS\n==================================================\nRandom: {'Precision@K': 0.010600000000000002, 'Recall@K': 0.0028904381753796027, 'NDCG@K': 0.010929535752877022}\nPopularity: {'Precision@K': 0.1968, 'Recall@K': 0.06704184790838692, 'NDCG@K': 0.21951699133470318}\nItemCF: {'Precision@K': 0.32120000000000004, 'Recall@K': 0.133772256259019, 'NDCG@K': 0.3615290254134801}\nSVD: {'Precision@K': 0.077, 'Recall@K': 0.024069515666332416, 'NDCG@K': 0.08446872786598195}\nNeural: {'Precision@K': 0.07479297780722094, 'Recall@K': 0.023560072267141373, 'NDCG@K': 0.0845653725908253}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================\n# 12. SAMPLE RECOMMENDATIONS\n# ============================================================================\nuser_id = 123\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"SAMPLE RECOMMENDATIONS FOR USER {user_id}\")\nprint(\"=\"*50)\nprint(\"Random:\", recommend_movies(user_id, N=10, method=\"random\"))\nprint(\"Popularity:\", recommend_movies(user_id, N=10, method=\"popular\"))\nprint(\"ItemCF:\", recommend_movies(user_id, N=10, method=\"itemcf\"))\nprint(\"SVD:\", recommend_movies(user_id, N=10, method=\"svd\"))\nprint(\"Neural:\", recommend_movies(user_id, N=10, method=\"neural\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:36:39.702514Z","iopub.execute_input":"2025-10-04T15:36:39.702778Z","iopub.status.idle":"2025-10-04T15:36:39.945545Z","shell.execute_reply.started":"2025-10-04T15:36:39.702761Z","shell.execute_reply":"2025-10-04T15:36:39.944664Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nSAMPLE RECOMMENDATIONS FOR USER 123\n==================================================\nRandom: [(188, 'Prophecy, The (1995)'), (1551, 'Buddy (1997)'), (3821, 'Nutty Professor II: The Klumps (2000)'), (1381, 'Grease 2 (1982)'), (2290, 'Stardust Memories (1980)'), (535, 'Short Cuts (1993)'), (2519, 'House on Haunted Hill (1958)'), (2053, 'Honey, I Blew Up the Kid (1992)'), (1798, 'Hush (1998)'), (1585, 'Love Serenade (1996)')]\nPopularity: [(110, 'Braveheart (1995)'), (2396, 'Shakespeare in Love (1998)'), (1197, 'Princess Bride, The (1987)'), (2997, 'Being John Malkovich (1999)'), (2628, 'Star Wars: Episode I - The Phantom Menace (1999)'), (1, 'Toy Story (1995)'), (3578, 'Gladiator (2000)'), (919, 'Wizard of Oz, The (1939)'), (541, 'Blade Runner (1982)'), (34, 'Babe (1995)')]\nItemCF: [(1036, 'Die Hard (1988)'), (1197, 'Princess Bride, The (1987)'), (733, 'Rock, The (1996)'), (919, 'Wizard of Oz, The (1939)'), (1220, 'Blues Brothers, The (1980)'), (2100, 'Splash (1984)'), (1374, 'Star Trek: The Wrath of Khan (1982)'), (1084, 'Bonnie and Clyde (1967)'), (380, 'True Lies (1994)'), (2134, 'Weird Science (1985)')]\nSVD: [(2019, 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'), (1949, 'Man for All Seasons, A (1966)'), (1172, 'Cinema Paradiso (1988)'), (3469, 'Inherit the Wind (1960)'), (2067, 'Doctor Zhivago (1965)'), (1797, 'Everest (1998)'), (1262, 'Great Escape, The (1963)'), (3479, 'Ladyhawke (1985)'), (1148, 'Wrong Trousers, The (1993)'), (2565, 'King and I, The (1956)')]\nNeural: [(1883, 'Bulworth (1998)'), (1263, 'Deer Hunter, The (1978)'), (1834, 'Spanish Prisoner, The (1997)'), (253, 'Interview with the Vampire (1994)'), (3386, 'JFK (1991)'), (3148, 'Cider House Rules, The (1999)'), (2819, 'Three Days of the Condor (1975)'), (1396, 'Sneakers (1992)'), (1131, 'Jean de Florette (1986)'), (912, 'Casablanca (1942)')]\n","output_type":"stream"}],"execution_count":19}]}